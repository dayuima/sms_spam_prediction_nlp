{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Milestones 2 (Phase 2)"
      ],
      "metadata": {
        "id": "bUkpi2dDuNRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Perkenalan\n",
        "\n",
        "- Nama  : Ida Ayu Gede Ima Dewi Pertami\n",
        "- Batch : HCK-004\n",
        "\n",
        "Problems Statement :\n",
        "- Spam SMS telah menjadi masalah besar dalam beberapa waktu terakhir. Hal ini dapat menyebabkan berbagai masalah seperti kehilangan informasi penting, pelanggaran privasi, dan juga kerugian finansial. Mendeteksi dan menyaring pesan spam ini adalah tugas penting bagi operator jaringan seluler dan platform pesan. \n",
        "\n",
        "\n",
        "Objective : \n",
        "- Untuk mengembangkan model pembelajaran mesin yang dapat mengklasifikasikan pesan SMS dengan akurat sebagai spam atau bukan spam\n",
        "\n",
        "\n",
        "Latar belakang :\n",
        "- Meningkatnya penggunaan telepon seluler telah menyebabkan peningkatan pesan spam, yang tidak hanya menjengkelkan tetapi juga merupakan ancaman potensial terhadap keamanan dan privasi pengguna. Operator jaringan seluler dan platform pesan perlu melindungi pelanggannya dari pesan seperti itu. Dataset koleksi spam SMS memberikan kesempatan besar untuk mengembangkan model pembelajaran mesin yang dapat mengklasifikasikan pesan SMS dengan akurat sebagai spam atau bukan spam. Tujuan dari proyek ini adalah membangun model klasifikasi spam SMS yang kuat yang dapat dengan akurat mendeteksi dan menyaring pesan spam secara real-time."
      ],
      "metadata": {
        "id": "FL8gMo-wuQdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Import Libraries"
      ],
      "metadata": {
        "id": "Z7fyMvSAugKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagian ini hanya berisi library yang digunakan dalam project"
      ],
      "metadata": {
        "id": "yV6RMRVbujqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Package installer for python\n",
        "!pip install feature_engine\n",
        "!pip install tensorflow\n",
        "!pip install pysastrawi"
      ],
      "metadata": {
        "id": "bvKRPVk_uq0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library untuk memanggil dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Libraries untuk exploratory data analysis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing data library\n",
        "import nltk\n",
        "import phik\n",
        "import string\n",
        "import re\n",
        "import ast\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Input, LSTM, GRU, Dropout\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "\n",
        "# Evaluasi\n",
        "from sklearn.metrics import accuracy_score,classification_report,roc_auc_score,ConfusionMatrixDisplay,confusion_matrix\n",
        "\n",
        "# Save model\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "JZQ4IhRhuxkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Data Loading"
      ],
      "metadata": {
        "id": "Oxuoc6NuvIII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagian ini berisi proses persiapan data sebelum dilakukan eksplorasi data lebih lanjut. Proses Loading Data dapat berupa pemberian nama baru untuk setiap kolom, pengecekan ukuran dataset, dll."
      ],
      "metadata": {
        "id": "tdVtM-xrvK4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengunggah file data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Qh5E1xDdvOtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset dan cek missing value non standard\n",
        "missing_values = [\"n/a\", \"na\", \"--\", \"none\", \"?\", \"-\",' ?', 'NaN', 'nan']\n",
        "df = pd.read_csv('spam.csv', na_values=missing_values , encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "YCh4yBt2wiwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Missing value pada dataset diganti menjadi nan value\n",
        "- Dalam contoh kode yang diberikan, encoding='ISO-8859-1' digunakan untuk memberitahu Python bahwa file CSV yang dibaca menggunakan encoding ISO-8859-1. Tanpa spesifikasi encoding ini, Python akan menggunakan encoding default yang mungkin tidak cocok dengan file CSV tersebut.\n",
        "- Pilihan encoding yang tepat sangat penting untuk membaca file CSV dengan benar, karena jika encoding tidak sesuai, karakter-karakter khusus atau tanda baca dalam teks dapat diartikan dengan salah dan memengaruhi analisis yang dilakukan."
      ],
      "metadata": {
        "id": "EvGX8WjFqSMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat jumlah baris dan kolom pada dataset\n",
        "df.shape"
      ],
      "metadata": {
        "id": "pGHG4Flqxb35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dari output diatas, dapat diinterpretasikan bahwa DataFrame memiliki 5.572 baris dan 5 kolom. Artinya, DataFrame terdiri dari 5.572 data (entri) yang masing-masing memiliki 5 fitur (kolom) yang berbeda. Informasi ini penting untuk membantu pemahaman tentang ukuran data yang digunakan dalam suatu analisis atau model pembelajaran mesin, serta dalam melakukan operasi data manipulasi, pemrosesan, dan visualisasi data yang sesuai."
      ],
      "metadata": {
        "id": "Tl18MdOJqWgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan 5 baris dataset teratas\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VNf4fAuCwmnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan 5 baris dataset terakhir\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "_g_bcZohwvJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memeriksa informasi dasar dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "DjC85MXBxigQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari dataset yang duplikasi\n",
        "df[df.duplicated()].shape"
      ],
      "metadata": {
        "id": "hLzYgwBzxnlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Berdasarkan hasil di atas menunjukkan jumlah baris dan kolom dari dataset dalam DataFrame yang memiliki duplikat atau data yang sama persis.\n",
        "- Hasil output menunjukkan bahwa terdapat 403 baris dan 5 kolom dalam dataset yang ditemukan sebagai duplikat.\n",
        "- Informasi ini dapat membantu dalam melakukan pembersihan data dengan menghapus atau menggabungkan data duplikat yang tidak diperlukan."
      ],
      "metadata": {
        "id": "W6YBeWPOrmCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan letak dataset yang terduplikasi\n",
        "df[df.duplicated()]"
      ],
      "metadata": {
        "id": "ojCIVKe6xsay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus dataset yang terduplikasi\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "XXIvYXPCxzIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan kembali dataset yang terduplikasi \n",
        "df[df.duplicated()].shape"
      ],
      "metadata": {
        "id": "alZB3egKx115"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tidak terdapat adanya dataset yang terdupliksi lagi setelah data duplikasi dihandling dengan cara di drop/dihapus\n",
        "- Data duplikat pada umumnya perlu dihapus atau di-drop dari dataset karena hal ini dapat mempengaruhi kinerja model dalam belajar dan menghasilkan output yang tidak akurat."
      ],
      "metadata": {
        "id": "FQGRpv86r000"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari missing value pada setiap kolom dalam dataset\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "nqQAHi0_x5FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Berdasarkan hasil di atas, dapat dilihat bahwa kolom 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4' memiliki  missing value. Jumlah missing value pada setiap kolom tersebut adalah 5126, 5159, 5164\n",
        "- Hal ini menunjukkan bahwa data pada kolom tersebut perlu diproses lebih lanjut untuk mengisi missing value atau menghapus baris yang memiliki missing value agar tidak mengganggu kualitas model yang akan dibangun. "
      ],
      "metadata": {
        "id": "rT5QMQIQr7d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat persentase missing value pada masing-masing kolom dan menghitung total persentase missing values pada dataset\n",
        "missing_percentage = (df.isna().sum() / len(df)) * 100\n",
        "print(missing_percentage)\n",
        "\n",
        "total_missing = df.isna().sum().sum()\n",
        "percentage_missing = (total_missing / (df.shape[0] * df.shape[1])) * 100\n",
        "print(f\"Persentase keseluruhan missing values di DataFrame adalah: {percentage_missing:.2f}%\")"
      ],
      "metadata": {
        "id": "rNipTyQuyA7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Persentase missing values dari kolom Unnamed: 2 , Unnamed: 3  dan Unnamed: 4  secara berturut-turut nilai missing valuesnya yaitu 99.168118, 99.806539 , dan 99.903269, hampir keseluruhan informasi yang ada pada kolom tersebut merupakan missing values maka akan dihandling dengan cara di hapus/drop missing values untuk kolom tersebut.\n",
        "- Missing value diatas menurut saya merupakan missing values MCAR (Missing Completely At Random) adalah jenis missing value dimana nilai yang hilang sepenuhnya acak, dan tidak terkait dengan nilai dari variabel lain dalam dataset. Artinya, kemungkinan terjadinya missing value di suatu observasi tidak dipengaruhi oleh nilai dari variabel manapun dalam dataset, dan tidak ada pola atau alasan tertentu yang menjelaskan terjadinya missing value.\n",
        "\n"
      ],
      "metadata": {
        "id": "LYIpKWVhsUcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus baris pada kolom 'Unnamed: 2','Unnamed: 3','Unnamed: 4' yang terdapat missing value \n",
        "df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Qc3aMAqTyPkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat missing values pada masing-masing kolom\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "_6uBbmMQu3j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Setelah missing values di handling dengan cara di trimming  maka tidak ada lagi missing values pada dataset"
      ],
      "metadata": {
        "id": "zZSut61eu0g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "HLjKZyLIzRDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagian ini berisi eksplorasi data pada dataset di atas dengan menggunakan query, pengelompokan, visualisasi sederhana, dan sebagainya.\n",
        "- Dikutip dari medium.com, Exploratory Data Analysis (EDA) merupakan bagian dari proses data science. EDA sangat penting sebelum melakukan feature engineering dan modeling karena pada tahap ini kita harus memahami data terlebih dahulu.\n",
        "Untuk EDA, saya sajikan beberapa visualisasi histogram dan visualisasi untuk informasi data kategorik berupa diagram batang dan diagram lingkaran."
      ],
      "metadata": {
        "id": "_2OuO5hszZEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah nama kolom\n",
        "df.rename(columns={'v1': 'label', 'v2': 'message'}, inplace=True)"
      ],
      "metadata": {
        "id": "pbR76sr_0hxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan deskriptif dari kolom-kolom pada DataFrame yang memiliki tipe data numerik\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "XBTjWrQv1RBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil di atas dapat dijelaskan sebagai berikut:\n",
        "\n",
        "- Kolom \"label\" memiliki 2 nilai unik (\"ham\" dan \"spam\").\n",
        "\"ham\" muncul sebanyak 4516 kali pada kolom \"label\", yang menunjukkan bahwa mayoritas pesan dalam dataset adalah pesan normal (\"ham\").\n",
        "- Kolom \"message\" juga memiliki 5169 nilai unik yang berbeda, yang menunjukkan bahwa setiap pesan dalam dataset unik dan tidak ada duplikasi."
      ],
      "metadata": {
        "id": "Tt0O-j7kxFpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat deskripsi kolom pada DataFrame yang telah dikelompokkan berdasarkan nilai pada kolom 'label\n",
        "df.groupby('label').describe().T"
      ],
      "metadata": {
        "id": "DgcJq2fF1uqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah pelanggan berdasarkan label\n",
        "df.label.value_counts().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "CfD8cv7y2PQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat plot untuk kolom label\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'label', data = df)\n",
        "plt.title('Number of ham and spam messages')"
      ],
      "metadata": {
        "id": "xGKh9vGJ7aBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari hasil tersebut, dapat dijelaskan sebagai berikut:\n",
        "- Terdapat 4516 pesan yang memiliki nilai \"ham\" pada kolom \"label\", dan 653 pesan yang memiliki nilai \"spam\" pada kolom \"label\".\n",
        "- Pesan dengan nilai \"ham\" memiliki kemunculan terbanyak dengan jumlah 4516.\n",
        "- Pesan dengan nilai \"spam\" memiliki kemunculan lebih sedikit dibanding pesan dengan nilai \"ham\", dengan jumlah 653.\n",
        "- Hasil ini memberikan gambaran umum tentang distribusi nilai pada kolom \"label\" dalam dataset, yaitu bahwa mayoritas pesan dalam dataset memiliki nilai \"ham\". Hal ini menunjukkan bahwa dataset yang digunakan mungkin tidak seimbang (imbalanced), di mana jumlah pesan dengan nilai \"ham\" jauh lebih banyak dibanding jumlah pesan dengan nilai \"spam\"."
      ],
      "metadata": {
        "id": "XeVLw2lfzTjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Feature Engineering/Data Preprocessing"
      ],
      "metadata": {
        "id": "zPGR25HjkOPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagian ini berisi proses penyiapan data untuk proses pelatihan model, seperti pembagian data menjadi train-val-test, transformasi data (normalisasi, encoding, dll.), dan proses-proses lain yang dibutuhkan."
      ],
      "metadata": {
        "id": "8aW74AePkS9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -Preprocessing Single Document-"
      ],
      "metadata": {
        "id": "YxwR4C0wkHYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langkah-langkah Preprocessing :\n",
        "\n",
        "- Mengubah teks ke lowercase\n",
        "- Menghilangkan tanda baca\n",
        "- Menghilangkan karakter yang tidak diperlukan\n",
        "- Menghilangkan stopwords\n",
        "- Stemming"
      ],
      "metadata": {
        "id": "TapvCcos-7Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat variabel baru dari DataFrame\n",
        "nba = df.copy()"
      ],
      "metadata": {
        "id": "4I1pxoOukew8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengganti nilai ham menjadi 0 dan spam menjadi 1\n",
        "nba['label'] = nba['label'].replace({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "49mbFqE5PxWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengambil 1 teks message\n",
        "sample = nba['message'].iloc[1]\n",
        "sample"
      ],
      "metadata": {
        "id": "xEr4wbrwkcR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open chatwords.txt\n",
        "with open('chatwords.txt') as j:\n",
        "    data = j.read()\n",
        "\n",
        "chatwords = ast.literal_eval(data)\n",
        "chatwords"
      ],
      "metadata": {
        "id": "cx7WW7hIo2qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Variabel chatwords berisi kamus yang dapat digunakan untuk membantu mengubah dataset (message) slang menjadi kata yang benar\n",
        "(sumber : https://www.kaggle.com/code/niteshk97/nlp-text-preprocessing#Step-5--Chat-word)"
      ],
      "metadata": {
        "id": "RrGTuTA6S6yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan penggantian kata-kata dalam sebuah string yang sesuai dengan kamus 'chatwords'\n",
        "temp=[]\n",
        "for chat in sample.split():\n",
        "   if chat.upper() in chatwords:\n",
        "      temp.append(chatwords[chat.upper()])\n",
        "   else:\n",
        "      temp.append(chat)\n",
        "\n",
        "sample = \" \".join(temp)\n",
        "sample"
      ],
      "metadata": {
        "id": "uV6K2Eb32DvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengganti semua huruf menjadi huruf kecil\n",
        "sample = sample.lower()\n",
        "sample"
      ],
      "metadata": {
        "id": "LpAdPz3NJrPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open abbreviation.txt\n",
        "with open('abbreviation.txt') as abb:\n",
        "    ab = abb.read()\n",
        "\n",
        "abbreviation =  ast.literal_eval(ab)\n",
        "abbreviation"
      ],
      "metadata": {
        "id": "zBUCSAHXJwtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- File teks yang berisi kamus abbreviation dan mengubahnya menjadi objek dictionary yang akan digunakan untuk menyesuaikan kata di dalam message\n",
        "(sumber : https://www.kaggle.com/code/life2short/data-processing-replace-abbreviation-of-word/notebook)"
      ],
      "metadata": {
        "id": "MANcRvNFUCjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan penggantian kata-kata dalam sebuah string yang sesuai dengan kamus 'abbreviation'\n",
        "temp2=[]\n",
        "for ab2 in sample.split():\n",
        "   if ab2 in abbreviation:\n",
        "      temp2.append(abbreviation[ab2])\n",
        "   else:\n",
        "      temp2.append(ab2)\n",
        "\n",
        "sample = \" \".join(temp2)\n",
        "sample"
      ],
      "metadata": {
        "id": "AVAoC59BKgpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghilangkan seluruh tanda baca\n",
        "sample = re.sub(\"[^a-zA-Z]\",' ', sample)\n",
        "sample = re.sub('\\[[^]]*\\]', ' ', sample)\n",
        "sample"
      ],
      "metadata": {
        "id": "FsGWu9j1KyXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghilangkan baris baru\n",
        "sample = re.sub(r\"\\\\n\", \" \", sample)\n",
        "# Menghilangkan whitespace\n",
        "sample = sample.strip()\n",
        "\n",
        "# Teks yang sudah bersih dari tanda baca\n",
        "sample = ' '.join(sample.split())\n",
        "sample"
      ],
      "metadata": {
        "id": "rhsWcowzK-yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "SrCYgnK1Lpvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stopwords digunakan untuk menghilangkan kata-kata yang umum agar tidak mempengaruhi hasil analisis atau klasifikasi yang dilakukan pada teks.\n",
        "- Kata-kata stopwords seperti \"the\", \"a\", \"an\", \"and\", \"in\", \"of\", dan lain-lain biasanya dihilangkan dari dokumen atau teks agar tidak mempengaruhi akurasi klasifikasi."
      ],
      "metadata": {
        "id": "jgrwodgOWR8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghilangkan stopwords\n",
        "tokens = word_tokenize(sample)\n",
        "stop_words2 = ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "print('Document       (Size :', len(sample.split()),') : ', sample,'\\n')\n",
        "print('Tokens         (Size :', len(tokens),') : ', tokens,'\\n')\n",
        "print('Cleaned Tokens (Size :', len(stop_words2.split()),') : ', stop_words2)"
      ],
      "metadata": {
        "id": "mxvD-ZkSLunB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menampilkan output yang terdiri dari tiga baris teks, yaitu:\n",
        "- Baris pertama menampilkan string sample asli (6 kata)\n",
        "- Baris kedua menampilkan token-token dari string sample (6 kata)\n",
        "- Baris ketiga menampilkan token-token dari string sample setelah proses penghapusan stop words (4 kata, with dan you dihilangkan)"
      ],
      "metadata": {
        "id": "J5fHV2-cV2OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi stemming\n",
        "ps = PorterStemmer()\n",
        "example_ps = [ps.stem(word) for word in stop_words2.split()]\n",
        "# Normalisasi lemmatization\n",
        "lem = WordNetLemmatizer()\n",
        "example_lem = [lem.lemmatize(word) for word in stop_words2.split()]\n",
        "\n",
        "stem = pd.DataFrame({'Original':stop_words2.split(),'Stemming':example_ps,'Lemmatization':example_lem})\n",
        "stem.head(7)"
      ],
      "metadata": {
        "id": "lWBtBnTZL6Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hasil di atas menunjukkan perbandingan antara original kata dengan kata yang telah melalui proses stemming dan lemmatization.\n",
        "- Kata \"joking\" mengalami perubahan menjadi \"joke\" dalam proses stemming dan tetap sama \"joking\" dalam proses lemmatization, karena kata \"joke\" merupakan bentuk dasar dari kata \"joking\"."
      ],
      "metadata": {
        "id": "Kq9PDF2xW83M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -Preprocessing Whole Document-"
      ],
      "metadata": {
        "id": "-3GOix5ZMqud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat fungsi digunakan untuk membersihkan teks yang tidak terstruktur untuk proses analisis teks lebih lanjut\n",
        "def check_chatwords(text):\n",
        "    temp=[]\n",
        "    for chat in text.split():\n",
        "        if chat.upper() in chatwords:\n",
        "            temp.append(chatwords[chat.upper()])\n",
        "        else:\n",
        "            temp.append(chat)\n",
        "    return \" \".join(temp)\n",
        "\n",
        "def lower(text):\n",
        "    data = text.lower()\n",
        "    return data \n",
        "\n",
        "def check_abbr(text):\n",
        "    temp2=[]\n",
        "    for abbr in text.split():\n",
        "      if abbr in abbreviation:\n",
        "          temp2.append(abbreviation[abbr])\n",
        "      else:\n",
        "          temp2.append(abbr)\n",
        "\n",
        "    return \" \".join(temp2)\n",
        "\n",
        "def check_punctuation(text):\n",
        "    data = re.sub(\"[^a-zA-Z]\",' ', text)\n",
        "    data = re.sub('\\[[^]]*\\]', ' ', data)\n",
        "    data = re.sub(r\"\\\\n\", \" \", data)\n",
        "    data = data.strip()\n",
        "    data = ' '.join(data.split())\n",
        "    return data   \n",
        "\n",
        "def token_stopwords_lemma(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words2 = ' '.join([word for word in tokens if word not in stop_words])\n",
        "    data = [lem.lemmatize(word) for word in stop_words2.split()]\n",
        "    data = ' '.join(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "_BHCl5ouMh2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- check_chatwords: Mengecek apakah kata dalam teks terdapat dalam kamus chatwords. Jika kata tersebut ada dalam kamus, maka kata akan diganti dengan kata pengganti dalam kamus, jika tidak, maka kata akan tetap sama.\n",
        "- lower: Mengubah seluruh huruf dalam teks menjadi huruf kecil.\n",
        "- check_abbr: Mengecek apakah kata dalam teks merupakan singkatan yang terdapat dalam kamus abbreviation. Jika kata tersebut adalah singkatan yang ada dalam kamus, maka kata akan diganti dengan kata yang sesuai dalam kamus, jika tidak, maka kata akan tetap sama.\n",
        "- check_punctuation: Menghapus seluruh tanda baca dalam teks.\n",
        "- token_stopwords_lemma: Memproses teks menjadi token, menghapus kata-kata yang terdapat dalam daftar stopwords, dan mengubah kata-kata dalam teks menjadi bentuk dasarnya menggunakan proses lemmatization."
      ],
      "metadata": {
        "id": "t9FY5Pm4YEv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membersihkan dan memproses kolom 'message' pada sebuah dataframe \n",
        "nba['message'] = nba['message'].apply(lambda j: check_chatwords(j))\n",
        "nba['message'] = nba['message'].apply(lambda k: lower(k))\n",
        "nba['message'] = nba['message'].apply(lambda v: check_abbr(v))\n",
        "nba['message'] = nba['message'].apply(lambda r: check_punctuation(r))\n",
        "nba['message'] = nba['message'].apply(lambda m: token_stopwords_lemma(m))"
      ],
      "metadata": {
        "id": "O5NwBQR7M2KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Setelah diproses melalui semua fungsi tersebut, kolom 'message' pada dataframe akan berisi teks yang sudah dibersihkan dan dipersiapkan untuk proses analisis teks lebih lanjut."
      ],
      "metadata": {
        "id": "WJhPS1B-ZIuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memperlihatkan 5 data yang sudah dibersihkan\n",
        "nba['message'].sample(6)"
      ],
      "metadata": {
        "id": "brgbec_AQbHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memperlihatkan 1 data full yang sudah dibersihkan\n",
        "nba['message'].iloc[0]"
      ],
      "metadata": {
        "id": "F8sn5U3BQy3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -Imbalance Handling-"
      ],
      "metadata": {
        "id": "ufDDEg8ARKmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total data pada kategori 1 adalah 653\n",
        "df_1 = nba[nba['label']==1]"
      ],
      "metadata": {
        "id": "PRZ8bODGRKTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maka total sample yang diambil pada kategori 0 adalah 653 juga\n",
        "df_0 = nba[nba['label']==0].sample(653,random_state=42)"
      ],
      "metadata": {
        "id": "8Gk5qFQgR3gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggabungkan kembali data yang sudah dilakukan imbalance handling\n",
        "nba2 = pd.concat([df_0,df_1],axis=0)\n",
        "nba2.shape"
      ],
      "metadata": {
        "id": "H3K8VGPLR5EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataframe nba2 terdiri dari dua kelompok data, yaitu data dengan label 0 dan data dengan label 1. \n",
        "- Kelompok data dengan label 1 tidak diubah, sedangkan kelompok data dengan label 0 diambil sejumlah 653 baris secara acak.\n",
        "- Tujuan dari pembuatan dataframe nba2 adalah untuk membuat dataset yang seimbang (balanced) antara kelompok data dengan label 0 dan label 1. \n",
        "- Dalam hal ini, jumlah data pada kedua kelompok sama besar, yaitu 653 baris. Sehingga jumlah total baris dalam dataframe nba2 adalah 1306.\n",
        "- Dataset yang seimbang sangat penting dalam proses klasifikasi karena jika kelompok data dengan label yang dominan terlalu banyak, maka kemungkinan model yang dibuat akan cenderung memprediksi label yang sama dengan label yang dominan tersebut."
      ],
      "metadata": {
        "id": "pKnwcCs3d2rL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -Data Splitting-"
      ],
      "metadata": {
        "id": "omfYw0CSTbhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # melakukan split pada data train dan data test\n",
        "# X_train_val, X_test, y_train_val, y_test = train_test_split(nba2['message'], nba2['label'], test_size = 0.2, random_state = 42, stratify=nba2['label'])\n",
        "# # melakukan split pada data train dan data validasi\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train_val,y_train_val,test_size=0.2, random_state = 42, stratify=y_train_val)"
      ],
      "metadata": {
        "id": "oJctEZUXS2X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Train, Test, Validasi\n",
        "X_train, X_test, y_train, y_test = train_test_split(nba2.message, \n",
        "                                                    nba2.label, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42, \n",
        "                                                    stratify=nba2.label)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "print('Train Size : ', X_train.shape)\n",
        "print('Test Size  : ', X_test.shape)\n",
        "print('Val Size  : ', X_val.shape)"
      ],
      "metadata": {
        "id": "_dnq1SkXZzzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -CountVectorizer-"
      ],
      "metadata": {
        "id": "gF3rhkcLTk5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Vocabularies\n",
        "Vectorize = CountVectorizer()\n",
        "X_train_vec = Vectorize.fit_transform(X_train)\n",
        "X_test_vec = Vectorize.transform(X_test)\n",
        "X_val_vec = Vectorize.transform(X_val)\n",
        "\n",
        "X_train_vec"
      ],
      "metadata": {
        "id": "B0InBLlYarZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  CountVectorizer menghasilkan sebuah matriks sparse dengan dimensi 835 baris dan 2508 kolom, dengan 8584 elemen yang disimpan \n",
        "- Matriks sparse tersebut merepresentasikan jumlah kemunculan setiap kata dalam dokumen yang telah diproses sebelumnya. Setiap baris mewakili satu dokumen, dan setiap kolom merepresentasikan satu kata. Angka pada matriks menunjukkan jumlah kemunculan kata tersebut pada dokumen tertentu."
      ],
      "metadata": {
        "id": "J23gJm6SfNn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencari Jumlah Vocab dan Panjang Token dalam Satu Document\n",
        "jml_vocab = len(Vectorize.vocabulary_.keys())\n",
        "max_len = max([len(i.split(\" \")) for i in X_train])\n",
        "\n",
        "print(\"Jumlah vocab : \", jml_vocab)\n",
        "print(\"Panjang maksimum kalimat : \", max_len, \"kata\")"
      ],
      "metadata": {
        "id": "whkPVPEKTxJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari hasil di atas, dapat dijelaskan bahwa:\n",
        "- jml_vocab menunjukkan jumlah kata unik dalam seluruh dataset setelah dilakukan proses vektorisasi menggunakan CountVectorizer.\n",
        "- max_len menunjukkan jumlah kata terbanyak yang ada dalam satu kalimat pada dataset setelah dilakukan proses preprocessing seperti menghapus stopwords dan tanda baca.\n",
        "- Terdapat 2508 kata unik dalam seluruh dataset dan panjang maksimum kalimat dalam dataset ini adalah 45 kata setelah dilakukan proses preprocessing."
      ],
      "metadata": {
        "id": "y-tNu0wkfsbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -Tokenization & Word Embedding-"
      ],
      "metadata": {
        "id": "C1c6g9VeULn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "text_vectorization = TextVectorization(max_tokens=jml_vocab,\n",
        "                                       standardize=\"lower_and_strip_punctuation\",\n",
        "                                       split=\"whitespace\",\n",
        "                                       ngrams=None,\n",
        "                                       output_mode=\"int\",\n",
        "                                       output_sequence_length=max_len,\n",
        "                                       input_shape=(1,) \n",
        "                                       )\n",
        "\n",
        "text_vectorization.adapt(X_train)"
      ],
      "metadata": {
        "id": "PWoe60caUOQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Kode di atas digunakan untuk melakukan vektorisasi pada teks menggunakan TextVectorization dari Keras. \n",
        "- Parameter max_tokens dan output_sequence_length digunakan untuk menentukan jumlah kata unik yang akan diambil dan jumlah token maksimum dalam setiap teks.\n",
        "- standardize mengatur apakah teks akan ditransformasikan ke huruf kecil dan dihapus tanda baca, split mengatur metode pembagian token"
      ],
      "metadata": {
        "id": "NuKJ_C8BgYDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding\n",
        "embedding = Embedding(input_dim=jml_vocab, \n",
        "                      output_dim=128, \n",
        "                      input_length=max_len, \n",
        "                      embeddings_initializer=\"uniform\", \n",
        "                      mask_zero=True)"
      ],
      "metadata": {
        "id": "E1TDY7hvUVmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Layer embedding bertujuan untuk mengubah representasi kata yang diberikan dalam bentuk token menjadi representasi vektor dengan dimensi yang lebih rendah\n",
        "- input_dim adalah jumlah kata yang ada pada vocabulary atau jumlah token yang unik.\n",
        "- output_dim adalah dimensi dari embedding vektor yang dihasilkan. Semakin besar nilai output_dim, semakin banyak informasi yang dapat ditampung oleh embedding vektor.\n",
        "- input_length adalah panjang dari sequence input yang diberikan, yang harus sama dengan output_sequence_length dari TextVectorization layer.\n",
        "- embeddings_initializer adalah fungsi yang digunakan untuk menginisialisasi nilai dari embedding vektor.\n",
        "- mask_zero akan memask input yang bernilai nol pada sequence untuk menghindari input yang tidak valid.\n",
        "- Dalam metode uniform, bobot embedding diinisialisasi dengan nilai acak yang diambil dari distribusi seragam dengan rentang (-0.05, 0.05). Pendekatan ini digunakan karena nilai awal bobot yang lebih kecil dapat membantu menghindari nilai yang terlalu besar atau terlalu kecil pada setiap iterasi, dan memastikan bahwa algoritma pembelajaran dapat mencapai konvergensi dengan lebih cepa"
      ],
      "metadata": {
        "id": "Yq77FAeggzey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Model Definition"
      ],
      "metadata": {
        "id": "TwQcqUZGUgQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Penjelasan algoritma-algoritma model yang digunakan-\n",
        "1. LSTM (Long Short-Term Memory)\n",
        "- LSTM adalah model RNN yang dirancang untuk mengatasi masalah vanishing gradient pada jaringan RNN. LSTM memiliki struktur yang lebih kompleks daripada RNN standar, dan memiliki tiga gerbang (gate) yang berfungsi untuk mengatur aliran informasi pada jaringan yaitu forget gate, input gate, dan output gate.\n",
        "- Kelebihan dari LSTM adalah mampu mengatasi masalah vanishing gradient pada jaringan RNN dan mampu mempertahankan informasi jangka panjang pada input yang diberikan. \n",
        "- Kelemahannya adalah kompleksitasnya membuat proses pelatihan lebih lambat dan memerlukan sumber daya komputasi yang lebih besar.\n",
        "2. GRU (Gated Recurrent Unit)\n",
        "- GRU adalah varian dari model LSTM yang memiliki struktur yang lebih sederhana. GRU memiliki dua gerbang (reset gate dan update gate) yang berfungsi untuk mengatur aliran informasi pada jaringan. \n",
        "- Kelebihan dari GRU adalah memiliki struktur yang lebih sederhana sehingga proses pelatihan lebih cepat dan memerlukan sumber daya komputasi yang lebih sedikit\n",
        "- Kelemahannya adalah GRU mungkin tidak seefektif LSTM dalam mengatasi masalah vanishing gradient pada jaringan RNN dan mungkin tidak dapat mempertahankan informasi jangka panjang dengan baik."
      ],
      "metadata": {
        "id": "fopQIN7jhO-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "YAR-ii3CUqCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training dengan Menggunakan LSTM\n",
        "model = Sequential()\n",
        "model.add(text_vectorization)\n",
        "model.add(embedding)\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oJykskySUpfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- text_vectorization: layer ini digunakan untuk melakukan vektorisasi teks, yaitu mengubah teks menjadi vektor angka dengan panjang yang sama.\n",
        "- embedding: layer ini digunakan untuk melakukan embedding kata, yaitu mengubah vektor angka yang dihasilkan oleh text_vectorization menjadi representasi vektor dalam ruang dimensi yang lebih rendah.\n",
        "- LSTM: layer ini merupakan salah satu jenis arsitektur dari recurrent neural network (RNN) yang memiliki kemampuan untuk mengingat informasi dari waktu sebelumnya dan digunakan untuk memodelkan hubungan sekuensial pada data. Dalam kode di atas, digunakan 2 buah LSTM layer yang masing-masing memiliki 32 unit neuron.\n",
        "- Dropout: layer ini digunakan untuk mencegah overfitting pada model dengan secara acak mematikan beberapa neuron selama pelatihan.\n",
        "- Dense: layer ini merupakan layer terakhir yang mengeluarkan output dalam bentuk 1 atau 0, yang menandakan sentimen positif atau negatif.\n",
        "- Hasil total params menunjukan jumlah parameter yang digunakan dalam model LSTM yang telah dibangun, yaitu 349,985. Dari total tersebut, seluruhnya merupakan trainable parameter, yang artinya parameter tersebut dapat diubah saat proses training berlangsung untuk meningkatkan performa model. "
      ],
      "metadata": {
        "id": "DbMywTv-jzr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "-7tw-llEVCmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training dengan Menggunakan GRU\n",
        "model_gru = Sequential()\n",
        "model_gru.add(text_vectorization)\n",
        "model_gru.add(embedding)\n",
        "model_gru.add(GRU(32, return_sequences=True))\n",
        "model_gru.add(GRU(32))\n",
        "model_gru.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model_gru.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')\n",
        "\n",
        "model_gru.summary()"
      ],
      "metadata": {
        "id": "5X51jJS2U_NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hasil output menunjukkan bahwa model GRU memiliki total parameter sebanyak 342,945 dan seluruhnya dapat dilatih. Arsitektur model ini terdiri dari lapisan TextVectorization, lapisan Embedding, 2 lapisan GRU, dan lapisan Dense yang menghasilkan output dengan aktivasi sigmoid untuk klasifikasi biner.\n",
        "- Fungsi sigmoid umumnya digunakan pada model yang melakukan binary classification, karena outputnya akan selalu berada di rentang 0 hingga 1.\n",
        "- Fungsi binary_crossentropy ini umumnya digunakan pada binary classification task, karena akan menghitung loss berdasarkan perbedaan antara nilai target dengan output model pada setiap sampel, dan mengoptimalkan model untuk meminimalkan loss tersebut selama proses training\n",
        "- adam adalah algoritme optimizer yang digunakan untuk melakukan optimasi gradient pada model. Algoritme ini sangat populer karena efektif dan efisien dalam melakukan optimasi pada model deep learning dengan banyak parameter.\n",
        "- accuracy adalah metrik yang digunakan untuk mengukur performa model, terutama pada binary classification task. Metrik ini menghitung persentase prediksi benar dari semua sampel.\n",
        "- Parameter tersebut digunakan karena sudah terbukti berhasil pada banyak kasus binary classification task dan merupakan default parameter yang disarankan oleh keras. Selain itu, sigmoid dan binary crossentropy adalah kombinasi yang sangat umum digunakan pada binary classification, sedangkan adam dikenal sangat efektif dalam mengoptimasi model deep learning. Sedangkan accuracy merupakan metrik yang umum digunakan untuk mengukur performa binary classification model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay0kqSorjPVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. Model Training"
      ],
      "metadata": {
        "id": "jBaO0yrlVMCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menambahkan sebuah callback function pada model keras\n",
        "callbacks1 = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience= 3, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "ClIpYYTRVTX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Callback function yang ditambahkan adalah EarlyStopping, yang digunakan untuk menghentikan pelatihan model secara otomatis ketika nilai dari suatu metrik (dalam kasus ini val_accuracy) tidak membaik (stagnan) dalam beberapa epoch terakhir."
      ],
      "metadata": {
        "id": "sbkFan14ltIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM "
      ],
      "metadata": {
        "id": "4mEmhwftVd8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history_lstm = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=callbacks1)"
      ],
      "metadata": {
        "id": "yY4Q_9NKViIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dari output di atas, dapat dijelaskan bahwa model dilatih selama 5 epochs, dengan setiap epoch terdiri dari 27 batch (27/27), masing-masing batch diproses selama sekitar 82ms. Pada epoch ke-5, loss yang dihasilkan oleh model adalah sebesar 0.0438 dan akurasi sebesar 0.9964 pada data pelatihan. Sedangkan pada data validasi, loss yang dihasilkan adalah sebesar 0.3465 dan akurasi sebesar 0.9187. Selain itu, waktu yang dibutuhkan untuk melatih model adalah sekitar 43.4 detik (wall time).\n",
        "- Pada epoch ke-5, model memiliki nilai loss sebesar 0.0438 pada data training dan memiliki nilai akurasi sebesar 0.9964. Sedangkan pada data validasi, model memiliki nilai loss sebesar 0.3465 dan nilai akurasi sebesar 0.9187.\n",
        "- Loss adalah suatu metrik yang digunakan untuk mengukur seberapa baik performa model dalam memprediksi target yang benar. Semakin rendah nilai loss, semakin baik performa model. Pada contoh di atas, dapat dilihat bahwa pada data training, model memiliki nilai loss yang sangat rendah, yaitu 0.0438. Namun, pada data validasi, model memiliki nilai loss yang lebih tinggi, yaitu 0.3465. Hal ini menunjukkan adanya overfitting, di mana model terlalu beradaptasi pada data training dan tidak dapat melakukan generalisasi pada data yang belum pernah dilihat sebelumnya.\n",
        "- Sedangkan akurasi merupakan metrik yang digunakan untuk mengukur seberapa baik performa model dalam mengklasifikasikan data dengan benar. Semakin tinggi nilai akurasi, semakin baik performa model. Pada contoh di atas, dapat dilihat bahwa pada data training, model memiliki nilai akurasi yang sangat tinggi, yaitu 0.9964. Namun, pada data validasi, model memiliki nilai akurasi yang lebih rendah, yaitu 0.9187. Hal ini juga menunjukkan adanya overfitting."
      ],
      "metadata": {
        "id": "hA6uMVSel8pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "Zi6IvAZ1V6Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history_gru = model_gru.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=callbacks1)"
      ],
      "metadata": {
        "id": "tp0-CQfgV8QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dari output di atas, dapat dijelaskan bahwa pada epoch ke-4, model mencapai loss sebesar 0.0174 dan akurasi sebesar 99.64% saat dilatih pada data pelatihan (train set). Pada saat evaluasi dengan menggunakan data validasi (validation set), model mencapai loss sebesar 0.2712 dan akurasi sebesar 94.26%. CPU time yang digunakan selama epoch ke-4 sebesar 24 detik dan waktu yang digunakan secara keseluruhan sebesar 21.3 detik (wall time).\n",
        "- Pada epoch 4, nilai loss pada data train adalah 0.0174 dan akurasi 0.9964 sedangkan pada data validasi, nilai lossnya 0.2712 dan akurasi 0.9426. Artinya, pada data train model mampu memprediksi dengan baik (akurasi 99.64%) dan loss yang rendah (0.0174), sedangkan pada data validasi performanya menurun sedikit (akurasi 94.26%) dan loss lebih tinggi (0.2712). Ini menunjukkan bahwa model mungkin mengalami overfitting karena performanya pada data train lebih baik daripada pada data validasi."
      ],
      "metadata": {
        "id": "tWYFlbR3nysE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIII. Model Evaluation"
      ],
      "metadata": {
        "id": "jX4ntklIWCyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM "
      ],
      "metadata": {
        "id": "q8MnbIxgW6Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_df = pd.DataFrame(history_lstm.history)"
      ],
      "metadata": {
        "id": "miNUUf8zXKMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_df[['accuracy', 'val_accuracy']].plot()"
      ],
      "metadata": {
        "id": "9otjLltVXlqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model_df[['loss', 'val_loss']].plot()"
      ],
      "metadata": {
        "id": "harWOtplXnf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Baik hasil loss dan acuraccy mengalami overfitting dan vanishing gradient"
      ],
      "metadata": {
        "id": "hjDB806QtBjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = model.predict(X_test)\n",
        "y_pred2 = np.where(y_pred2 >=0.5, 1, 0)\n",
        "print(classification_report(y_test, y_pred2))"
      ],
      "metadata": {
        "id": "6mP1FykQcVQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hasil tersebut menunjukkan bahwa model memiliki kinerja yang baik untuk kedua kelas, dengan nilai precision dan recall yang tinggi untuk kedua kelas dan f1-score rata-rata yang cukup tinggi, yaitu 0.96."
      ],
      "metadata": {
        "id": "Sr2xRcz7pn38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm2 = tf.math.confusion_matrix(labels=y_test, predictions=y_pred2)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cm2, annot=True,fmt = 'd')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")"
      ],
      "metadata": {
        "id": "0yc2z3XEccc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dari confusion matrix di atas, dapat dilihat bahwa model mampu memprediksi dengan benar sebanyak 128 data kelas 0 dan 124 data kelas 1. Namun, terdapat 7 data kelas 1 yang salah diprediksi sebagai kelas 0 dan 3 data kelas 0 yang salah diprediksi sebagai kelas 1."
      ],
      "metadata": {
        "id": "EmYUgEQYpyVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "3RaEiupBcCHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_model2_df = pd.DataFrame(history_gru.history)"
      ],
      "metadata": {
        "id": "gOiqiC0zcn-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model2_df[['accuracy', 'val_accuracy']].plot()"
      ],
      "metadata": {
        "id": "dRqLc8stcpmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model2_df[['loss', 'val_loss']].plot()"
      ],
      "metadata": {
        "id": "cZHCXS00cspv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Baik hasil loss dan acuraccy mengalami overfitting dan vanishing gradient"
      ],
      "metadata": {
        "id": "sx6sYLmktMhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3 = model_gru.predict(X_test)\n",
        "y_pred3 = np.where(y_pred3 >=0.5, 1, 0)\n",
        "print(classification_report(y_test, y_pred3))"
      ],
      "metadata": {
        "id": "o75l1gfLcvlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hasil dari classification_report menunjukkan performa model GRU yang cukup baik dengan akurasi sebesar 0.97. Precision, recall, dan f1-score pada kedua kelas (0 dan 1) juga memiliki nilai yang cukup tinggi, yaitu 97 ke atas. \n",
        "- Dapat disimpulkan bahwa model GRU dapat melakukan klasifikasi dengan baik pada dataset SMS Spam, sehingga model GRU dipilih sebagai model saving."
      ],
      "metadata": {
        "id": "htRV6qRNqX5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm3 = tf.math.confusion_matrix(labels=y_test, predictions=y_pred3)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cm3, annot=True,fmt = 'd')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")"
      ],
      "metadata": {
        "id": "vn0BNgaccx2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pada confusion matrix, kita dapat melihat bahwa model GRU memprediksi 127 data spam dan 4 data ham sebagai spam (false positive) dan 3 data spam dan 128 data ham sebagai ham (false negative). Namun secara keseluruhan, hasil prediksi model GRU memiliki tingkat keakuratan yang cukup tinggi dan dapat diandalkan."
      ],
      "metadata": {
        "id": "jutwIbOwqtrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IX. Model Saving"
      ],
      "metadata": {
        "id": "SM-Tb_Lrc1XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze Model\n",
        "model_gru.trainable = False\n",
        "model_gru.summary()"
      ],
      "metadata": {
        "id": "RLpFQboBe9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Preprocessing\n",
        "with open('chatwords.pkl', 'wb') as file_1:\n",
        "  joblib.dump(check_chatwords, file_1)\n",
        "with open('lowercase.pkl', 'wb') as file_2:\n",
        "  joblib.dump(lower, file_2)\n",
        "with open('abbreviation.pkl', 'wb') as file_3:\n",
        "  joblib.dump(check_abbr, file_3)\n",
        "with open('punctuation.pkl', 'wb') as file_4:\n",
        "  joblib.dump(check_punctuation, file_4)\n",
        "with open('stopwords_lemma.pkl', 'wb') as file_5:\n",
        "  joblib.dump(token_stopwords_lemma, file_5)"
      ],
      "metadata": {
        "id": "IcWnb9Iffqa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model \n",
        "model_gru.save('model_gru')"
      ],
      "metadata": {
        "id": "KXapVD37gFCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X. Kesimpulan"
      ],
      "metadata": {
        "id": "MCNSwIgydU6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset SMS Spam adalah dataset yang tidak seimbang (imbalanced), dan pada kasus klasifikasi, penanganan (handling) data yang tidak seimbang ini diperlukan untuk meningkatkan performa model. \n",
        "- Dataset yang seimbang sangat penting dalam proses klasifikasi karena jika kelompok data dengan label yang dominan terlalu banyak, maka kemungkinan model yang dibuat akan cenderung memprediksi label yang sama dengan label yang dominan tersebut.\n",
        "- Performa model GRU lebih baik dari LSTM, dilihat dari classification_report menunjukkan performa model GRU yang cukup baik dengan akurasi sebesar 0.97. Precision, recall, dan f1-score pada kedua kelas (0 dan 1) juga memiliki nilai yang cukup tinggi, yaitu 97 ke atas. Sehingga dapat disimpulkan bahwa model GRU dapat melakukan klasifikasi dengan baik pada dataset SMS Spam.\n",
        "- Dalam bisnis, dapat disimpulkan bahwa model GRU dapat digunakan untuk membantu proses klasifikasi SMS menjadi spam atau tidak spam dengan cukup baik. Hal ini dapat membantu perusahaan dalam mengelola pesan-pesan yang masuk ke dalam sistem mereka, sehingga dapat lebih efektif dan efisien dalam menjawab pesan-pesan yang memang perlu dijawab dan meminimalkan waktu yang terbuang pada pesan-pesan spam yang tidak relevan.\n",
        "- Namun baik model LSTM dan GRU pada dataset SMS spam mengalami overfitting dan vanishing gradient, maka perlu dilakukan evaluasi dan optimasi lebih lanjut pada arsitektur dan hiperparameter model untuk memperbaiki performa model. Solusi yang dapat dilakukan untuk mengatasi vanishing gradien adalah menggunakan fungsi aktivasi lain seperti Elu, Selu, Leaky dll, dan bisa juga mengganti weight intializer seperti : Glorot, He, Random, Lecun dll, dan juga menggunakan Batch normalization."
      ],
      "metadata": {
        "id": "vU527xCmdU2y"
      }
    }
  ]
}